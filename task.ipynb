{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2989bacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.0.339-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.23.5)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.9.0-cp310-cp310-win_amd64.whl (364 kB)\n",
      "     -------------------------------------- 364.2/364.2 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (3.5.0)\n",
      "Collecting langsmith<0.1.0,>=0.0.63\n",
      "  Downloading langsmith-0.0.66-py3-none-any.whl (46 kB)\n",
      "     -------------------------------------- 46.8/46.8 kB 335.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (6.0.1)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.28.1)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-win_amd64.whl (44 kB)\n",
      "     -------------------------------------- 44.4/44.4 kB 726.5 kB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.3-cp310-cp310-win_amd64.whl (75 kB)\n",
      "     ---------------------------------------- 75.6/75.6 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<4.0->langchain) (1.2.0)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "     -------------------------------------- 49.4/49.4 kB 313.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (22.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Installing collected packages: typing-inspect, tenacity, multidict, marshmallow, jsonpatch, frozenlist, async-timeout, yarl, dataclasses-json, aiosignal, langsmith, aiohttp, langchain\n",
      "Successfully installed aiohttp-3.9.0 aiosignal-1.3.1 async-timeout-4.0.3 dataclasses-json-0.6.2 frozenlist-1.4.0 jsonpatch-1.33 langchain-0.0.339 langsmith-0.0.66 marshmallow-3.20.1 multidict-6.0.4 tenacity-8.2.3 typing-inspect-0.9.0 yarl-1.9.3\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (1.3.4)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from openai) (2.4.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from openai) (0.25.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1.9.0->openai) (2.10.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyPDF2 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (3.0.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp310-cp310-win_amd64.whl (10.8 MB)\n",
      "     ---------------------------------------- 10.8/10.8 MB 6.7 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain#LangChain is an open source framework that lets software developers working with artificial intelligence (AI) and its machine learning subset combine large language models with other external components to develop LLM-powered applications          \n",
    "!pip install openai#The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+ application\n",
    "!pip install pyPDF2#PyPDF2 is a free and open-source pure-python PDF library capable of splitting, merging, cropping, and transforming the pages of PDF files.\n",
    "!pip install faiss-cpu#This is a library created by Facebook for similarity search. It's particularly good at quickly finding similar items in large datasets.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a97cec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (0.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\samwa\\appdata\\roaming\\python\\python310\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken#Tiktoken is an open-source tool developed by OpenAI that is utilized for tokenizing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de4e5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e2c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching API key\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] =\"""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b835a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading pdf file and put them into a variable called reader\n",
    "reader = PdfReader('2023_GPT4All_Technical_Report.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b89fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from the file and put them into a variable called raw_text\n",
    "raw_text = ''\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22aa4178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6021"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking length of raw text\n",
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cd0bbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nY'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing first 100 characters\n",
    "raw_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6613eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split the text that we read into smaller chunks so that during information retreival we don't hit the token size limits. \n",
    "\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 2500,\n",
    "    chunk_overlap  = 10,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10ee9b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7de042a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzanussbaum@gmail.com\\nBrandon Duderstadt\\nbrandon@nomic.aiBenjamin Schmidt\\nben@nomic.aiAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nThis preliminary technical report describes the\\ndevelopment of GPT4All, a chatbot trained\\nover a massive curated corpus of assistant in-\\nteractions including word problems, story de-\\nscriptions, multi-turn dialogue, and code. We\\nopenly release the collected data, data cura-\\ntion procedure, training code, and final model\\nweights to promote open research and repro-\\nducibility. Additionally, we release quantized\\n4-bit versions of the model allowing virtually\\nanyone to run the model on CPU.\\n1 Data Collection and Curation\\nWe collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-\\nple of questions/prompts by leveraging three pub-\\nlicly available datasets:\\n• The unified chip2 subset of LAION OIG.\\n• Coding questions with a random sub-sample\\nof Stackoverflow Questions\\n• Instruction-tuning with a sub-sample of Big-\\nscience/P3\\nWe chose to dedicate substantial attention to data\\npreparation and curation based on commentary in\\nthe Stanford Alpaca project (Taori et al., 2023).\\nUpon collection of the initial dataset of prompt-\\ngeneration pairs, we loaded data into Atlas for data\\ncuration and cleaning. With Atlas, we removed all\\nexamples where GPT-3.5-Turbo failed to respond\\nto prompts and produced malformed output. This\\nreduced our total number of examples to 806,199\\nhigh-quality prompt-generation pairs. Next, we\\ndecided to remove the entire Bigscience/P3 sub-\\nset from the final training dataset due to its very\\nFigure 1: TSNE visualization of the candidate training\\ndata (Red: Stackoverflow, Orange: chip2, Blue: P3).\\nThe large blue balls (e.g. indicated by the red arrow)\\nare highly homogeneous prompt-response pairs.\\nlow output diversity; P3 contains many homoge-\\nneous prompts which produce short and homoge-\\nneous responses from GPT-3.5-Turbo. This exclu-\\nsion produces a final subset containing 437,605\\nprompt-generation pairs, which is visualized in\\nFigure 2. You can interactively explore the dataset\\nat each stage of cleaning at the following links:\\n• Cleaned with P3\\n• Cleaned without P3 (Final Training Dataset)\\n2 Model Training\\nWe train several models finetuned from an in-'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b3424bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stance of LLaMA 7B (Touvron et al., 2023).\\nThe model associated with our initial public re-\\nlease is trained with LoRA (Hu et al., 2021)\\non the 437,605 post-processed examples for four\\nepochs. Detailed model hyper-parameters and\\ntraining code can be found in the associated repos-\\nitory and model training log.(a) TSNE visualization of the final training data, ten-colored\\nby extracted topic.\\n(b) Zoomed in view of Figure 2a. The region displayed con-\\ntains generations related to personal health and wellness.\\nFigure 2: The final training data was curated to ensure a diverse distribution of prompt topics and model responses.\\n2.1 Reproducibility\\nWe release all data (including unused P3 genera-\\ntions), training code, and model weights for the\\ncommunity to build upon. Please check the Git\\nrepository for the most up-to-date data, training\\ndetails and checkpoints.\\n2.2 Costs\\nWe were able to produce these models with about\\nfour days work, $800 in GPU costs (rented from\\nLambda Labs and Paperspace) including several\\nfailed trains, and $500 in OpenAI API spend.\\nOur released model, gpt4all-lora, can be trained in\\nabout eight hours on a Lambda Labs DGX A100\\n8x 80GB for a total cost of $100 .\\n3 Evaluation\\nWe perform a preliminary evaluation of our model\\nusing the human evaluation data from the Self-\\nInstruct paper (Wang et al., 2022). We report the\\nground truth perplexity of our model against what\\nis, to our knowledge, the best openly available\\nalpaca-lora model, provided by user chainyo on\\nhuggingface. We find that all models have very\\nlarge perplexities on a small number of tasks, and\\nreport perplexities clipped to a maximum of 100.\\nModels finetuned on this collected dataset ex-\\nhibit much lower perplexity in the Self-Instruct\\nevaluation compared to Alpaca. This evaluation is\\nin no way exhaustive and further evaluation work\\nFigure 3: Model Perplexities. Lower is better. Our\\nmodels achieve stochastically lower ground truth per-\\nplexities than alpaca-lora.\\nremains. We welcome the reader to run the model\\nlocally on CPU (see Github for files) and get a\\nqualitative sense of what it can do.\\n4 Use Considerations\\nThe authors release data and training details in\\nhopes that it will accelerate open LLM research,\\nparticularly in the domains of alignment and inter-\\npretability. GPT4All model weights and data are\\nintended and licensed only for research purposes\\nand any commercial use is prohibited. GPT4All\\nis based on LLaMA, which has a non-commercial'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85bccc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'license. The assistant data is gathered from Ope-\\nnAI’s GPT-3.5-Turbo, whose terms of use pro-hibit developing models that compete commer-\\ncially with OpenAI.\\nReferences\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\\nlarge language models.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\\nford alpaca: An instruction-following llama\\nmodel. https://github.com/tatsu-lab/\\nstanford_alpaca .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,\\nBaptiste Rozi `ere, Naman Goyal, Eric Hambro,\\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\\nEdouard Grave, and Guillaume Lample. 2023.\\nLlama: Open and efficient foundation language\\nmodels.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\\nguage model with self generated instructions.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb35c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embeddings from OpenAI\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69d58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docsearch = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25362978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0ba55",
   "metadata": {},
   "source": [
    "Loads a chain that you can use to do QA over a set of documents, but it uses ALL of those documents.\n",
    "\n",
    "chain_type=\"stuff\" will not work because the number of tokens exceeds the limit. We can try other chain types like \"map_reduce\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffeb17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#created object chain of class load_qa_chain\n",
    "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f70489a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' $800 in GPU costs (rented from Lambda Labs and Paperspace) including several failed trains, and $500 in OpenAI API spend.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What was the cost of training the GPT4all model?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c032d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The model was trained with LoRA (Hu et al., 2021) on the 437,605 post-processed examples for four epochs. Detailed model hyper-parameters and training code can be found in the associated repository and model training log.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How was the model trained?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43bc8adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The final training dataset contained 437,605 prompt-generation pairs.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what was the size of the training dataset?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "346c1cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This model is different from other models because it is trained over a massive curated corpus of assistant interactions including word problems, story descriptions, multi-turn dialogue, and code. Additionally, the authors have released quantized 4-bit versions of the model allowing it to be run on CPU.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How is this different from other models?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "69fb89a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I don't know.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"who is the prime minister of india?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff0823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
